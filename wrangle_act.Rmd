---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.6
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %autosave 0
```

```{python}
import pandas as pd
import numpy as np
import requests
import matplotlib.image as mpimg
import csv
import io
import matplotlib.pyplot as plt
from pprint import pprint
# %matplotlib inline
```

# Project: Gathering and analyze data of  WeRateDogs dataset

```{python}
img=mpimg.imread('dog.jpg')
imgplot = plt.imshow(img)
```

## Gathering information
**Read the provided file twitter-archive-enhanced-2.csv from the local folder**

```{python}
tw_arc = pd.read_csv('twitter-archive-enhanced-2.csv')
```

**Download file using the provided link** 

```{python}
r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv').content
prediction = pd.read_csv(io.StringIO(r.decode('utf-8')), sep = '\t')
prediction.to_csv('prediction.tsv')
print('prediction shape', prediction.shape)
prediction.head()
```

**Access tweets via API with tweet_id**

```{python}
import tweepy
import tweepy
from tweepy import OAuthHandler
import json
from timeit import default_timer as timer
import os

if os.path.isfile('tweet_json.txt'):
    print("The dataset has been downloaded already and written into the tweet_json.txt file. The data will be extracted from this file below")
else:
# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file
# These are hidden to comply with Twitter's API terms and conditions

    consumer_key = 'hzkXG5HqgXN6UcAAaDE4ykLdm'  #'YOUR CONSUMER KEY'
    consumer_secret = 'YLq3cp6W3rrshh8wPwCX7sLswKgiw5RkFyxj0JDzInODhp2MAR'  # 'YOUR CONSUMER SECRET'
    access_token = '60896767-HDMh9m1vUbCexrXiWnqtijslouuisAAUEDcWn6Wu8' #'YOUR ACCESS TOKEN'
    access_secret = 'oo5wfzXEyzSKa8Y763CTBvpK2JkI5TVDQv5jL040dc1Oz'  #'YOUR ACCESS SECRET'

    auth = OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_secret)

    api = tweepy.API(auth, wait_on_rate_limit=True)

# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:
# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to
# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv
# NOTE TO REVIEWER: this student had mobile verification issues so the following
# Twitter API code was sent to this student from a Udacity instructor
# Tweet IDs for which to gather additional data via Twitter's API
    tweet_ids = tw_arc.tweet_id.values
    len(tweet_ids)

# Query Twitter's API for JSON data for each tweet ID in the Twitter archive
    count = 0
    fails_dict = {}
    start = timer()
# Save each tweet's returned JSON as a new line in a .txt file
    with open('tweet_json.txt', 'w') as outfile:
    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit
        for tweet_id in tweet_ids:
            count += 1
            print(str(count) + ": " + str(tweet_id))
            try:
                tweet = api.get_status(tweet_id, tweet_mode='extended')
                print("Success")
                json.dump(tweet._json, outfile)
                outfile.write('\n')
            except tweepy.TweepError as e:
                print("Fail")
                fails_dict[tweet_id] = e
                pass
    end = timer()
    print(end - start)
    print(fails_dict)
```

```{python}
tweets = []
for line in open('tweet_json.txt', 'r'):
    tweets.append(json.loads(line))
#print out one json object for further reference 
tweets[0]     
```

```{python}
#creat a list of data from tweet_json.txt
list_twits = []
for json_data in tweets:
    list_twits.append({'id' : json_data['id'],
                       'retweet_count': int(json_data['retweet_count']),
                       'favorite_count' : int(json_data['favorite_count'])})
   
#creat a Dataframe
tweets_api = pd.DataFrame(list_twits, columns = ['id', 'retweet_count' , 'favorite_count']) 
#Check out the obatined DataFrame
tweets_api.head() 
```

## Data assessment

```{python}
tw_arc
```

```{python}
tw_arc.info()
```

```{python}
tw_arc.describe()
```

```{python}
tw_arc.rating_numerator.value_counts().head(25) 
```

```{python}
tw_arc.in_reply_to_status_id.value_counts().head()
```

```{python}
tw_arc.source.value_counts()
```

```{python}
tw_arc.doggo.value_counts()
```

```{python}
tw_arc.floofer.value_counts()                       
```

```{python}
tw_arc.pupper.value_counts()
```

```{python}
tw_arc.puppo.value_counts()
```

```{python}
tw_arc.name.value_counts().head()
```

```{python}
tw_arc.sample(10)
```

```{python}
prediction.head()
```

```{python}
prediction.info()
```

```{python}
prediction.img_num.value_counts()
```

```{python}
prediction.describe()
```

```{python}
tweets_api.info()
```

```{python}
tweets_api.describe()
```

**Note:** For me it looks like some information is missing. This variable should confirm if this is a true picture of a dog breed or not.  

```{python}
prediction.query('tweet_id == "674063288070742018"').jpg_url
```

```{python}
prediction.query('tweet_id == "674063288070742018"')
```

```{python}
img=mpimg.imread('straus.jpg')
imgplot = plt.imshow(img)
```

```{python}
prediction.query('tweet_id == "670822709593571328"').jpg_url
```

```{python}
prediction.query('tweet_id == "670822709593571328"')
```

```{python}
img=mpimg.imread('small_doggy.jpg')
imgplot = plt.imshow(img)
```

As we can see, one tweet has a straus on a picture and it was correctly classified as "not dog breed" while another picture has dog on it. We can see that p1_dog and p3_dog misclassified, i.e. it is not a dog breed, while p2_dog correctly classified this picture. However, we cannot confirm this information unless we open each picture, which is not possible due to a big voleume of data.   


**Quality**

*`tw_arc`* table
* column source in tw_arc is too long for such source information
* in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp variables have a lot of missing data and, moreover, we do not need them for the analysis
* rating_denominator is 10 in 2333 cases out 2356 cases, consider rating_denominator to be 10
* rating_numerator in most cases is in between 0 and 15, the rest consider as outliers
* variable name has 745 None values and 55 "a" values 
* in timestamp +0000 is redundent information 
* name variable has some entries starting with low case letters(example: An). Is An a dog name? It occurs 7 time in the dataset, though.
* expanded_urls contains link which are not valid, possible because they are expired
* remove rows in retweet_count and favorite_count with missing values

*`prediction`* table
* variable img_num is not needed  

* change variable types to the appropriate, where it is needed 
* prediction table is missing one importent varaible which would show if the picture truly contains breed of dog or not like it is shown above. one picture has straus on it and algotithm classified it as it is not a breed of dog, but another picture has a dog on it; however, it was misclassified as not breed of dog

**Tidiness**

*`tw_arc`* table
* Variables doggo, floofer, pupper and puppo in one column
* tables wt_arc and tweets_api form one observational unit 

*`prediction`* table

* jpg_url variable should be in tw_arc table to satisfy tidiness definition 

* tw_arc and prediction tables form two different obseravation units and will be kept seperatly 



## Cleaning data

```{python}
#first make copies of datasets: 

tw_arc_original = tw_arc.copy()
prediction_original = prediction.copy()
tweets_api_original = tweets_api.copy()
```

**Quality**


**`Define`**

URLs in source columns of tw_arc table shows source. Change provided URLs to the corresponding 4 categories  


**`Code`**

```{python}
#We want to change this list: 
source_list = ['<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>',
              '<a href="http://vine.co" rel="nofollow">Vine - Make a Scene</a>',
              '<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>',
              '<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>']
#to this list:
new_source_list = ['Twitter for iPhone', 'Vine', 'Twitter Web Client', 'TweetDeck']
```

```{python}
[tw_arc.source.replace(source_list, new_source_list, inplace=True) for current_source, new_source in zip(source_list, new_source_list)];
```

**`Test`**

```{python}
tw_arc.source.sample(10)
```

**`Define`**

drop these variables: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp  in tw_acr

```{python}
tw_arc.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1, inplace=True )

```

**`Test`**

```{python}
tw_arc.head()
```

**`Define`**

Let rating_denominator to be 10 for all entries. Hence, because all the entries have only one value 10, then drop this varible. You don't want to have the entire column with all zeros.    


**`Code`**

```{python}
tw_arc.drop('rating_denominator', axis=1, inplace=True)
```

**`Test`**

```{python}
tw_arc.head(1)
```

**`Define`**

If rating_numerator is higer than 15 let it be 15. Assume that 15 is the maximum possible value. Everything higher are outliers. 


**`Code`**

```{python}
tw_arc.loc[tw_arc['rating_numerator']>15, 'rating_numerator'] = 15
```

**`Test`**

```{python}
tw_arc.describe()
```

**`Define`**

in tw_arc name 'a' and None change to NaN


**`Code`**

```{python}
tw_arc.name.replace(['None', 'a'], np.nan, inplace=True)
```

**`Test`**

```{python}
tw_arc.name.value_counts().head()
```

**`Define`**

Remove +0000 from timestamp 


**`Code`**

```{python}
tw_arc.timestamp = tw_arc.timestamp.str[:-5].str.strip()
```

**`Test`**

```{python}
tw_arc.timestamp.head()
```

**`Define`**

Convert lowcase names in variable name to Title


**`Code`**

```{python}
tw_arc.name =tw_arc.name.str.title()
```

**`Test`**

```{python}
tw_arc.name.sample(5)
```

**`Define`**

Drop img_num from prediction table 


**`Code`**

```{python}
prediction.drop('img_num', axis = 1, inplace = True)

```

**`Test`**

```{python}
prediction.head(1)
```

**`Define`**

Variables doggo, floofer, pupper and puppo move to one variable called classifier


**`Code`**

```{python}
tw_arc_temp=tw_arc[~((tw_arc['doggo'] != 'doggo') & (tw_arc['floofer'] != 'floofer') & (tw_arc['pupper'] != 'pupper') & (tw_arc['puppo'] != 'puppo'))].replace('None', np.nan)



```

```{python}
tw_arc_temp = pd.melt(tw_arc_temp, id_vars = ['tweet_id', 'timestamp', 'source', 'text', 'expanded_urls', 'rating_numerator', 'name'],
                      var_name = 'var_name', value_name = 'classifier')
```

```{python}
tw_arc_temp.sample(5)
```

```{python}
tw_arc_temp.dropna(subset=['classifier'], inplace = True)
```

```{python}
tw_arc_temp.sample(5)
```

```{python}
#Before we merge it with tw_arc get rid of redundent variables in both tables 
tw_arc_temp.drop(['timestamp', 'source', 'text', 'expanded_urls', 'rating_numerator', 'name', 'var_name'], axis = 1, inplace=True)
```

```{python}
tw_arc_temp.sample(5)
```

```{python}
tw_arc.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis = 1 , inplace = True)
```

```{python}
tw_arc.sample(5)
```

```{python}
tw_arc = tw_arc.merge(tw_arc_temp, on='tweet_id', how='left')

```

```{python}
tw_arc.classifier.fillna('not classified', inplace=True)
```

**`Test`**

```{python}
tw_arc.sample(5)
```

```{python}
tw_arc.info()
```

```{python}
tw_arc.classifier.sample(10)
```

**`Define`**

Merge tw_arc and tweets_api tables together


**`Code`**

```{python}
tw_arc.head(1)
```

```{python}
tweets_api.head(1)
```

```{python}
tw_arc = tw_arc.merge(tweets_api, how='left', left_on='tweet_id', right_on='id')
```

```{python}
tw_arc.head(1)
```

```{python}
#Remove redundent variable id
tw_arc.drop('id', axis=1, inplace = True)
```

**`Test`**

```{python}
tw_arc.head(1)
```

**`Define`**

Move jpg_url from prediction to tw_arc and drop jpg_url


**`Code`**

```{python}
prediction.head()
```

```{python}
tw_arc = tw_arc.merge(prediction, on='tweet_id', how="left")
```

```{python}
tw_arc.drop(["p1", 'p1_conf','p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog'], axis = 1, inplace = True)
prediction.drop('jpg_url', axis = 1, inplace = True)
```

**`Test`**

```{python}
tw_arc.head(1)
```

```{python}
prediction.head(1)
```

**`Define`**

Drop expanded_urls 

**`Code`**

```{python}
tw_arc.drop('expanded_urls', axis=1, inplace=True)
```

**`Test`**

```{python}
tw_arc.head(1)
```

**`Define`**

Change variable types to appropriate


**`Code`**

```{python}
tw_arc.info()
```

**We can see that retweet_count and favorite_count have  14 missing values. Let's remove rows with missing values.**

```{python}
tw_arc.dropna(subset = ['retweet_count', 'favorite_count'], inplace=True )
```

```{python}
tw_arc.tweet_id = tw_arc.tweet_id.astype(str)
tw_arc.timestamp = pd.to_datetime(tw_arc.timestamp)    
tw_arc.source = tw_arc.source.astype(str)
tw_arc.text = tw_arc.text.astype(str)
tw_arc.name = tw_arc.name.astype(str)
tw_arc.classifier = tw_arc.classifier.astype('category')
tw_arc.retweet_count = tw_arc.retweet_count.astype('int64')
tw_arc.favorite_count = tw_arc.favorite_count.astype('int64')
tw_arc.jpg_url = tw_arc.jpg_url.astype(str)
```

**``Test``**

```{python}
tw_arc.info()
```

```{python}
#where info gives object check a cell:
type(tw_arc.tweet_id[0])
```

```{python}
type(tw_arc.source[0])
```

```{python}
type(tw_arc.text[0])
```

```{python}
type(tw_arc.text[0])
```

```{python}
type(tw_arc.jpg_url[0])
```

```{python}
tw_arc.head()
```

**Because preidction table does not have a variable which shows if it is truly a breed of dog or not as it was shown above we cannot extract information how effective this algorithm. Because of this we won't keep this table** 


## Store files to csv 

```{python}
tw_arc.to_csv('twitter_archive_master.csv')
prediction.to_csv('twitter_archive_prediction.csv')
```
## Analyzing and Visualizing Data

```{python}
tw_arc.head()
```




```{python}
tw_arc.describe()
```

```{python}
fig, ax = plt.subplots()
plt.hist(tw_arc.rating_numerator);
plt.title('Rating numerator histogram');
ax.set_ylabel('count');
ax.set_xlabel('rating value');
```

*Let's define a confidence interval for the mean of the rating_numerator using a bootstrapping method*

```{python}
sample = tw_arc.rating_numerator.sample(300)
```

```{python}
sample_means = np.random.choice(sample, size=(10000, 200)).mean(axis=1)
sample_means
```

```{python}
left, right = np.percentile(sample_means, 2.5), np.percentile(sample_means, 97.5), 
plt.hist(sample_means);
plt.axvline(left, color='r');
plt.axvline(right, color='r');
```

```{python}
print("We can expect the mean of our population to be in between {} and {}".format(left, right))
```

Plot histogram for retweet_count

```{python}
plt.hist(tw_arc.retweet_count);
```

We can say that this histogram is skewed left and has outliers. Plot boxplot to define outliers:

```{python}
plt.boxplot(tw_arc.retweet_count);
```

We can see that outliers are above 5000. Plot hist for values less then 5000: 

```{python}
box_rc = tw_arc[tw_arc.retweet_count < 5000]
fig, ax = plt.subplots()
plt.hist(box_rc.retweet_count);
plt.title('Retweet count histogram');
ax.set_ylabel('count');
ax.set_xlabel('Number of retweets');
```

Calculate mean and standard deviation:

```{python}
print('mean: {}, standard deviation: {}'.format(np.mean(tw_arc.retweet_count), np.std(tw_arc.retweet_count)))
```

Plot hist for favorite_count

```{python}
plt.hist(tw_arc.favorite_count);
```

Outliers are presented here 

```{python}
plt.boxplot(tw_arc.favorite_count);
```

Assume that outliers are everything above 25000

```{python}
box_fc = tw_arc[tw_arc.favorite_count < 25000]
```

```{python}
plt.hist(box_fc.favorite_count);
```

Calculate mean and standard deviation:

```{python}
print('mean: {}, standard deviation: {}'.format(np.mean(tw_arc.favorite_count), np.std(tw_arc.favorite_count)))
```

Investigate correlations between parameters

1. between retweet_count and favorite_count

```{python}
import seaborn as sns #https://stackoverflow.com/questions/37234163/how-to-add-a-line-of-best-fit-to-scatter-plot
mask = ((tw_arc['retweet_count']<5000) & (tw_arc['favorite_count']<25000))
sns.regplot(tw_arc[mask].retweet_count, tw_arc[mask].favorite_count)
plt.title('Correlation between favorite count and retweet count');
```

favorite_counte = 0 we can consider as outliers 

```{python}
mask = (tw_arc['favorite_count']!=0) & (tw_arc['retweet_count']<5000) & (tw_arc['favorite_count']<25000)
```

```{python}
sns.regplot(tw_arc[mask].retweet_count, tw_arc[mask].favorite_count);
plt.title('Correlation between favorite count and retweet count');
```

```{python}
#calculate correlation coeffcients: 
np.corrcoef(tw_arc[mask].retweet_count, tw_arc[mask].favorite_count)[0,1]
```

Hence, we have a very strong, positive correlation 

2. Correlation between retweet_count and rating_numerator

```{python}
sns.regplot(tw_arc[mask].retweet_count, tw_arc[mask].rating_numerator);
plt.title('Correlation between rating numerator and retweet count');
```

```{python}
#calculate correlation coeffcients: 
np.corrcoef(tw_arc[mask].retweet_count, tw_arc[mask].rating_numerator)[0,1]
```

Hence, these parameters have weak, positive correlation 

```{python}
sns.regplot(tw_arc[mask].favorite_count, tw_arc[mask].rating_numerator);
plt.title('Correlation between rating numerator and favorite count');
```

```{python}
np.corrcoef(tw_arc[mask].favorite_count, tw_arc[mask].rating_numerator)[0,1]
```

Hence, these parameters have weak, positive correlation 

Let's plot bar charts for Source and Classifier variables 

```{python}
counts = [tw_arc.query('source == "Twitter for iPhone"').count()[0], tw_arc.query('source == "Vine"').count()[0],
          tw_arc.query('source == "Twitter Web Client"').count()[0], tw_arc.query('source == "TweetDeck"').count()[0]]
ind = np.arange(1, 5)
fig, ax = plt.subplots();
plt.bar(ind, counts);
ax.set_xticks(ind);
ax.set_xticklabels(new_source_list);
ax.set_ylabel('Counts');
ax.set_title('Source');
```

```{python}
doggo_list= ['not classified','doggo', 'floofer', 'pupper', 'puppo']
counts = [tw_arc.query('classifier == "not classified"').count()[0], tw_arc.query('classifier == "doggo"').count()[0], tw_arc.query('classifier == "floofer"').count()[0],
          tw_arc.query('classifier == "floofer"').count()[0], tw_arc.query('classifier == "puppo"').count()[0]]
ind = np.arange(1, 6)
fig, ax = plt.subplots();
plt.bar(ind, counts);
ax.set_xticks(ind);
ax.set_xticklabels(doggo_list);
ax.set_ylabel('Counts');
ax.set_title('Dog classification');
```

```{python}

```
